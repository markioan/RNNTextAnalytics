{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Keras & Tensorflow\n",
    "!pip install --upgrade tensorflow\n",
    "!pip install Keras\n",
    "!pip install keras-tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & unzip fasttext word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "!gzip -d cc.en.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "idx = 0\n",
    "vocab = {}\n",
    "with open(\"cc.en.300.vec\", 'r', encoding=\"utf-8\", newline='\\n',errors='ignore') as f:\n",
    "    for l in f:\n",
    "        line = l.rstrip().split(' ')\n",
    "        if idx == 0:\n",
    "            vocab_size = int(line[0]) + 2\n",
    "            dim = int(line[1])\n",
    "            vecs = np.zeros(vocab_size*dim).reshape(vocab_size,dim)\n",
    "            vocab[\"__PADDING__\"] = 0\n",
    "            vocab[\"__UNK__\"] = 1\n",
    "            idx = 2\n",
    "        else:\n",
    "            vocab[line[0]] = idx\n",
    "            emb = np.array(line[1:]).astype(np.float)\n",
    "            if (emb.shape[0] == dim):\n",
    "                vecs[idx,:] = emb\n",
    "                idx+=1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    pickle.dump(vocab,open(\"fasttext_voc\",'wb'))\n",
    "    np.save(\"fasttext.npy\",vecs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "fasttext_embed = np.load(\"fasttext.npy\")\n",
    "fasttext_word_to_index = pickle.load(open(\"fasttext_voc\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    Source\n",
    "    ------\n",
    "    https://github.com/fchollet/keras/issues/5400#issuecomment-314747992\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"Calculate the F1 score.\"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r))\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               c#\n",
       "1          asp.net\n",
       "2      objective-c\n",
       "3             .net\n",
       "4           python\n",
       "5          asp.net\n",
       "6        angularjs\n",
       "7           iphone\n",
       "8    ruby-on-rails\n",
       "9        angularjs\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('stack-overflow-data.csv')\n",
    "data['tags'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "javascript       2000\n",
       "sql              2000\n",
       "jquery           2000\n",
       "php              2000\n",
       "html             2000\n",
       "c                2000\n",
       "iphone           2000\n",
       "c#               2000\n",
       "asp.net          2000\n",
       ".net             2000\n",
       "ios              2000\n",
       "ruby-on-rails    2000\n",
       "android          2000\n",
       "css              2000\n",
       "objective-c      2000\n",
       "c++              2000\n",
       "python           2000\n",
       "mysql            2000\n",
       "angularjs        2000\n",
       "java             2000\n",
       "Name: tags, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tags.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer \n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_train, test = train_test_split(data,\n",
    "                                    test_size=0.3,\n",
    "                                    random_state=1596,\n",
    "                                    stratify=data['tags'])\n",
    "train, train_dev = train_test_split(full_train,\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=1596,\n",
    "                                    stratify=full_train['tags'])\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "X_train =train['post']\n",
    "X_test = test['post']\n",
    "X_train_dev = train_dev['post']\n",
    "\n",
    "y_train = mlb.fit_transform(train['tags'])\n",
    "y_test = mlb.transform(test['tags'])\n",
    "y_train_dev = mlb.transform(train_dev['tags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22400, 25)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to sequence of indexes and PADDING\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_WORDS =20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = fasttext_embed.shape[1]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='__UNK__')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "train_seqs = tokenizer.texts_to_sequences(X_train)\n",
    "test_seqs = tokenizer.texts_to_sequences(X_test)\n",
    "dev_seqs = tokenizer.texts_to_sequences(X_train_dev)\n",
    "train_data = pad_sequences(train_seqs, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "test_data = pad_sequences(test_seqs, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "dev_data = pad_sequences(dev_seqs, maxlen=MAX_SEQUENCE_LENGTH,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 25)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  114   389     7  3903   395    31  1053   395    10    24   317     3\n",
      "    48   142     7   114     2     4 15410     4    14     4  1053     4\n",
      "   395   137   389    30   587   389     7     2     4  3903     4   395\n",
      "   200   703     2   419   389    14     2     4  1053     4   395   145\n",
      "    53  1511     3    48   453   449    25     3   506     3    48   350\n",
      "   220   328    10    24     4    89  3658   614   419   174    10     2\n",
      "     4  1053     4   395     9     4  1053     6     1  3903     6     1\n",
      "     4     9     3   221     2   247     4     9     4    18    35 14030\n",
      "    15 14030     5     1 15410   183 14030     1   389   114     1 15410\n",
      " 14030    49     1 15410 14030   402     4     9   162  1265     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "Found 100587 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(train_data[345])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model's embedding matrix\n",
    "embedding_matrix = np.zeros((MAX_WORDS+2, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_WORDS:\n",
    "            continue\n",
    "    try:\n",
    "        embedding_vector = fasttext_embed[fasttext_word_to_index[word],:]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20002, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and train a BiGRU (RNN) model with an MLP on top of it \n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Bidirectional\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "GRU_SIZE = 100\n",
    "DENSE = 200\n",
    "N_CLASSES = 25\n",
    "\n",
    "# create empty sequential model\n",
    "model = Sequential()\n",
    "# add an embedding layer\n",
    "model.add(Embedding(MAX_WORDS+2, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                    input_length=MAX_SEQUENCE_LENGTH,mask_zero=True, trainable=False))\n",
    "# Use 0.2 dropout probabillity\n",
    "model.add(Dropout(0.2))\n",
    "# add a bidirectional gru layer with 0.2 variational (recurrent) dropout \n",
    "model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=False, recurrent_dropout = 0.2)))\n",
    "# add a hidden MLP layer\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense( DENSE, activation='relu' ))\n",
    "# add the output MLP layer\n",
    "model.add(Dense( N_CLASSES, activation='sigmoid' ))\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=[precision, recall, f1, accuracy])\n",
    "\n",
    "checkpoint = ModelCheckpoint('keras_BiGRU_model', monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train_data, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=5,\n",
    "              verbose = 0,\n",
    "              callbacks=[checkpoint,TQDMNotebookCallback()],\n",
    "              validation_data=(dev_data, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize Model's training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# summarize history for f1\n",
    "plt.plot(history.history['f1'])\n",
    "plt.plot(history.history['val_f1'])\n",
    "plt.title('Model f1')\n",
    "plt.ylabel('f1-score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_prob = model.predict(dev_data, batch_size=32, verbose=0)\n",
    "print(metrics.classification_report(y_test, (y_prob > 0.5).astype('int32')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom keras layer for linear and deep self-attention over RNNs output states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.core import Layer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class LinearAttention(Layer):\n",
    "    def __init__(self,\n",
    "                 kernel_regularizer=None, bias_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.b_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "        super(LinearAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((1,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        if self.return_attention:\n",
    "            return [None, None]\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        # eij = Wx + b\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            eij *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # a = softmax(eij)\n",
    "        a = K.expand_dims(K.softmax(eij, axis=-1))\n",
    "        weighted_input = x * a\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]\n",
    "\n",
    "\n",
    "class DeepAttention(Layer):\n",
    "    def __init__(self,\n",
    "                 kernel_regularizer=None, u_regularizer=None, bias_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b1_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.b2_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b1_constraint = constraints.get(b_constraint)\n",
    "        self.b2_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "        super(DeepAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b1 = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b1'.format(self.name),\n",
    "                                     regularizer=self.b1_regularizer,\n",
    "                                     constraint=self.b1_constraint)\n",
    "            self.b2 = self.add_weight((1,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b2'.format(self.name),\n",
    "                                     regularizer=self.b2_regularizer,\n",
    "                                     constraint=self.b2_constraint)\n",
    "        else:\n",
    "            self.b1 = None\n",
    "            self.b2 = None\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        if self.return_attention:\n",
    "            return [None, None]\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # uit = tanh(Wx + b)\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b1\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "\n",
    "        # ait = softmax(Ueij)\n",
    "        eij = dot_product(uit, self.u)\n",
    "        if self.bias:\n",
    "            eij += self.b2\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            eij *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a = K.expand_dims(K.softmax(eij, axis=-1))\n",
    "        \n",
    "        weighted_input = x * a\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a Bi-LSTM + deep self-attention + MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 1000, 300)         6000600   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1000, 300)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 1000, 200)         320800    \n",
      "_________________________________________________________________\n",
      "deep_attention_9 (DeepAttent [(None, 200), (None, 1000 40401     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 25)                5025      \n",
      "=================================================================\n",
      "Total params: 6,407,026\n",
      "Trainable params: 406,426\n",
      "Non-trainable params: 6,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/hard/anaconda3/envs/text_analytics/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77f71bdfb55449db047e47609d660b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=5, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd37bbb71f40427ca29c0143b8ec8aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=22400, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-96fb6c2753e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTQDMNotebookCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m               shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/text_analytics/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/text_analytics/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text_analytics/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text_analytics/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text_analytics/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Bidirectional, Input\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "LSTM_SIZE = 100\n",
    "DENSE = 200\n",
    "N_CLASSES = 25\n",
    "\n",
    "inputs = Input((MAX_SEQUENCE_LENGTH,))\n",
    "embeddings = Embedding(MAX_WORDS+2,EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                    input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False)(inputs)\n",
    "drop_emb = Dropout(0.2)(embeddings)\n",
    "bilstm = Bidirectional(LSTM(units=LSTM_SIZE, return_sequences=True,recurrent_dropout = 0.2))(drop_emb)\n",
    "#x, attn = LinearAttention(return_attention=True)(bilstm)\n",
    "x, attn = DeepAttention(return_attention=True)(bilstm)\n",
    "out = Dense(units=DENSE, activation=\"relu\")(x)\n",
    "out = Dense(units=N_CLASSES, activation=\"sigmoid\")(out)\n",
    "model2 = Model(inputs, out)\n",
    "\n",
    "print(model2.summary())\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=[precision, recall, f1, accuracy])\n",
    "\n",
    "checkpoint = ModelCheckpoint('keras_BiLSTM+attn_model', monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history2 = model2.fit(train_data, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=5,\n",
    "              verbose = 0,\n",
    "              callbacks=[checkpoint,TQDMNotebookCallback()],\n",
    "              validation_data=(dev_data, y_train_dev),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1000)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Bidirectional, Input\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "LSTM_SIZE = 100\n",
    "DENSE = 200\n",
    "N_CLASSES = 25\n",
    "\n",
    "\n",
    "def load_biLSTM_model(x_train, y_train, x_train_dev, y_train_dev, params):\n",
    "    inputs = Input((MAX_SEQUENCE_LENGTH,))\n",
    "    embeddings = Embedding(MAX_WORDS+2,EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                        input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False)(inputs)\n",
    "    drop_emb = Dropout(0.2)(embeddings)\n",
    "    bilstm = Bidirectional(LSTM(units=LSTM_SIZE, return_sequences=True,recurrent_dropout = 0.2))(drop_emb)\n",
    "    #x, attn = LinearAttention(return_attention=True)(bilstm)\n",
    "    x, attn = DeepAttention(return_attention=True)(bilstm)\n",
    "    out = Dense(units=DENSE, activation=params['activation1'])(x) \n",
    "    out = Dense(units=N_CLASSES, activation=params['activation2'])(out) \n",
    "    model2 = Model(inputs, out)\n",
    "\n",
    "    print(model2.summary())\n",
    "    model2.compile(loss='binary_crossentropy',\n",
    "                      optimizer=params['optimizer'], \n",
    "                      metrics=[precision, recall, f1, accuracy])\n",
    "\n",
    "    checkpoint = ModelCheckpoint('keras_BiLSTM+attn_model', monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    history2 = model2.fit(x_train, y_train,\n",
    "                  batch_size=params['batch_size'],  \n",
    "                  epochs=params['epochs'],   \n",
    "                  verbose = 0,\n",
    "                  callbacks=[checkpoint,TQDMNotebookCallback()],\n",
    "                  validation_data=(x_train_dev, y_train_dev),\n",
    "                  shuffle=True)\n",
    "    \n",
    "    return history2,model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation1': 'relu', 'activation2': 'tanh', 'optimizer': 'Nadam', 'batch_size': 40, 'epochs': 5}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 1000, 300)         6000600   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1000, 300)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 1000, 200)         320800    \n",
      "_________________________________________________________________\n",
      "deep_attention_11 (DeepAtten [(None, 200), (None, 1000 40401     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 25)                5025      \n",
      "=================================================================\n",
      "Total params: 6,407,026\n",
      "Trainable params: 406,426\n",
      "Non-trainable params: 6,000,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4169bc9ea14e748f83db0a782f6ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=5, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=5600, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_f1 did not improve from -inf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=5600, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_f1 did not improve from -inf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788ce41adb954ac3b64481750208022b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=5600, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# params epoch, batch_size, activation1, activation2, loss?, optimizer\n",
    "# todo ftia3e synarthsh gia to modelo\n",
    "import talos\n",
    "import os\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "TALOS_DIR = os.path.join(DATA_DIR, 'talos_logs')\n",
    "TALOS_TF_LOG_FILENAME = 'talos_tf_log'\n",
    "talos_tf_log_pathname = os.path.join(TALOS_DIR, TALOS_TF_LOG_FILENAME)\n",
    "\n",
    "\n",
    "p = {'activation1':['relu', 'elu'],\n",
    "     'activation2':['sigmoid', 'tanh'],\n",
    "     'optimizer': ['Nadam', 'Adam'],\n",
    "     'batch_size': [20,30,40],\n",
    "     'epochs': [3]}\n",
    "\n",
    "talos.Scan(train_data, y_train, \n",
    "           x_val=dev_data, \n",
    "           y_val=y_train_dev, \n",
    "           model=load_biLSTM_model, \n",
    "           params=p,grid_downsample=0.1,\n",
    "           print_params=True,\n",
    "           seed=123,\n",
    "           last_epoch_value=True,\n",
    "           dataset_name=talos_tf_log_pathname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize Model's training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# summarize history for f1\n",
    "plt.plot(history2.history['f1'])\n",
    "plt.plot(history2.history['val_f1'])\n",
    "plt.title('Model f1')\n",
    "plt.ylabel('f1-score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}