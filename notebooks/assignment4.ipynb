{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Keras & Tensorflow\n",
    "!pip install --upgrade tensorflow\n",
    "!pip install Keras\n",
    "!pip install keras-tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & unzip fasttext word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "!gzip -d cc.en.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "idx = 0\n",
    "vocab = {}\n",
    "with open(\"cc.en.300.vec\", 'r', encoding=\"utf-8\", newline='\\n',errors='ignore') as f:\n",
    "    for l in f:\n",
    "        line = l.rstrip().split(' ')\n",
    "        if idx == 0:\n",
    "            vocab_size = int(line[0]) + 2\n",
    "            dim = int(line[1])\n",
    "            vecs = np.zeros(vocab_size*dim).reshape(vocab_size,dim)\n",
    "            vocab[\"__PADDING__\"] = 0\n",
    "            vocab[\"__UNK__\"] = 1\n",
    "            idx = 2\n",
    "        else:\n",
    "            vocab[line[0]] = idx\n",
    "            emb = np.array(line[1:]).astype(np.float)\n",
    "            if (emb.shape[0] == dim):\n",
    "                vecs[idx,:] = emb\n",
    "                idx+=1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    pickle.dump(vocab,open(\"fasttext_voc\",'wb'))\n",
    "    np.save(\"fasttext.npy\",vecs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "fasttext_embed = np.load(\"fasttext.npy\")\n",
    "fasttext_word_to_index = pickle.load(open(\"fasttext_voc\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    Source\n",
    "    ------\n",
    "    https://github.com/fchollet/keras/issues/5400#issuecomment-314747992\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"Calculate the F1 score.\"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r))\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('stack-overflow-data.csv')\n",
    "data['tags'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tags.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer \n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train, test = train_test_split(data, test_size = 0.3)\n",
    "\n",
    "X_train =train['post']\n",
    "X_test = test['post']\n",
    "\n",
    "\n",
    "y_train = mlb.fit_transform(train['tags'])\n",
    "y_test = mlb.transform(test['tags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to sequence of indexes and PADDING\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_WORDS =20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = fasttext_embed.shape[1]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='__UNK__')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "train_seqs = tokenizer.texts_to_sequences(X_train)\n",
    "dev_seqs = tokenizer.texts_to_sequences(X_test)\n",
    "train_data = pad_sequences(train_seqs, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "dev_data = pad_sequences(dev_seqs, maxlen=MAX_SEQUENCE_LENGTH,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[345])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model's embedding matrix\n",
    "embedding_matrix = np.zeros((MAX_WORDS+2, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_WORDS:\n",
    "            continue\n",
    "    try:\n",
    "        embedding_vector = fasttext_embed[fasttext_word_to_index[word],:]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and train a BiGRU (RNN) model with an MLP on top of it \n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Bidirectional\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "GRU_SIZE = 100\n",
    "DENSE = 200\n",
    "N_CLASSES = 25\n",
    "\n",
    "# create empty sequential model\n",
    "model = Sequential()\n",
    "# add an embedding layer\n",
    "model.add(Embedding(MAX_WORDS+2, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                    input_length=MAX_SEQUENCE_LENGTH,mask_zero=True, trainable=False))\n",
    "# Use 0.2 dropout probabillity\n",
    "model.add(Dropout(0.2))\n",
    "# add a bidirectional gru layer with 0.2 variational (recurrent) dropout \n",
    "model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=False, recurrent_dropout = 0.2)))\n",
    "# add a hidden MLP layer\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense( DENSE, activation='relu' ))\n",
    "# add the output MLP layer\n",
    "model.add(Dense( N_CLASSES, activation='sigmoid' ))\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=[precision, recall, f1, accuracy])\n",
    "\n",
    "checkpoint = ModelCheckpoint('keras_BiGRU_model', monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train_data, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=5,\n",
    "              verbose = 0,\n",
    "              callbacks=[checkpoint,TQDMNotebookCallback()],\n",
    "              validation_data=(dev_data, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize Model's training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# summarize history for f1\n",
    "plt.plot(history.history['f1'])\n",
    "plt.plot(history.history['val_f1'])\n",
    "plt.title('Model f1')\n",
    "plt.ylabel('f1-score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_prob = model.predict(dev_data, batch_size=32, verbose=0)\n",
    "print(metrics.classification_report(y_test, (y_prob > 0.5).astype('int32')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom keras layer for linear and deep self-attention over RNNs output states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.core import Layer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class LinearAttention(Layer):\n",
    "    def __init__(self,\n",
    "                 kernel_regularizer=None, bias_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.b_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "        super(LinearAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((1,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        if self.return_attention:\n",
    "            return [None, None]\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        # eij = Wx + b\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            eij *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # a = softmax(eij)\n",
    "        a = K.expand_dims(K.softmax(eij, axis=-1))\n",
    "        weighted_input = x * a\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]\n",
    "\n",
    "\n",
    "class DeepAttention(Layer):\n",
    "    def __init__(self,\n",
    "                 kernel_regularizer=None, u_regularizer=None, bias_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b1_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.b2_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b1_constraint = constraints.get(b_constraint)\n",
    "        self.b2_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "        super(DeepAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b1 = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b1'.format(self.name),\n",
    "                                     regularizer=self.b1_regularizer,\n",
    "                                     constraint=self.b1_constraint)\n",
    "            self.b2 = self.add_weight((1,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b2'.format(self.name),\n",
    "                                     regularizer=self.b2_regularizer,\n",
    "                                     constraint=self.b2_constraint)\n",
    "        else:\n",
    "            self.b1 = None\n",
    "            self.b2 = None\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        if self.return_attention:\n",
    "            return [None, None]\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # uit = tanh(Wx + b)\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b1\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "\n",
    "        # ait = softmax(Ueij)\n",
    "        eij = dot_product(uit, self.u)\n",
    "        if self.bias:\n",
    "            eij += self.b2\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            eij *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a = K.expand_dims(K.softmax(eij, axis=-1))\n",
    "        \n",
    "        weighted_input = x * a\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a Bi-LSTM + deep self-attention + MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Bidirectional, Input\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "LSTM_SIZE = 100\n",
    "DENSE = 200\n",
    "N_CLASSES = 25\n",
    "\n",
    "inputs = Input((MAX_SEQUENCE_LENGTH,))\n",
    "embeddings = Embedding(MAX_WORDS+2,EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                    input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False)(inputs)\n",
    "drop_emb = Dropout(0.2)(embeddings)\n",
    "bilstm = Bidirectional(LSTM(units=LSTM_SIZE, return_sequences=True,recurrent_dropout = 0.2))(drop_emb)\n",
    "#x, attn = LinearAttention(return_attention=True)(bilstm)\n",
    "x, attn = DeepAttention(return_attention=True)(bilstm)\n",
    "out = Dense(units=DENSE, activation=\"relu\")(x)\n",
    "out = Dense(units=N_CLASSES, activation=\"sigmoid\")(out)\n",
    "model2 = Model(inputs, out)\n",
    "\n",
    "print(model2.summary())\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=[precision, recall, f1, accuracy])\n",
    "\n",
    "checkpoint = ModelCheckpoint('keras_BiLSTM+attn_model', monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history2 = model2.fit(train_data, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=5,\n",
    "              verbose = 0,\n",
    "              callbacks=[checkpoint,TQDMNotebookCallback()],\n",
    "              validation_data=(dev_data, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize Model's training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# summarize history for f1\n",
    "plt.plot(history2.history['f1'])\n",
    "plt.plot(history2.history['val_f1'])\n",
    "plt.title('Model f1')\n",
    "plt.ylabel('f1-score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
